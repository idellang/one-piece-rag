{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# remove column width restrictions\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_path = os.path.join(ROOT, 'data')\n",
    "valid_characters_url = os.path.join(data_path, 'one_piece_characters_urls.txt')\n",
    "\n",
    "# read the file and store each line as an element in a list\n",
    "with open(valid_characters_url, 'r') as file:\n",
    "    valid_characters = [line.strip() for line in file.readlines()]\n",
    "\n",
    "print(f\"Number of valid characters: {len(valid_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# get random url from valid_characters\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "\n",
    "\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 Character Parser - jfcastaneda.led@gmail.com'\n",
    "    }\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "character_data = {'url': url}\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Parse infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = soup.find('aside', class_='portable-infobox')\n",
    "if not infobox:\n",
    "    print(f\"No infobox found for {url}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # The most reliable source is the infobox's main title\n",
    "    character_data['name'] = infobox.find('h2', class_='pi-title').get_text(strip=True)\n",
    "except AttributeError:\n",
    "    character_data['name'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    affiliations = None\n",
    "    \n",
    "    label_tag = infobox.find('h3', string=re.compile(\"Affiliations\"))\n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # Find all the <a> tags, which contain the actual affiliation names\n",
    "            affiliation_links = value_tag.find_all('a')\n",
    "            # Extract the clean text from each link\n",
    "            affiliation_names = [link.get_text(strip=True) for link in affiliation_links]\n",
    "            # Join them into a clean, comma-separated string\n",
    "            affiliations = \", \".join(affiliation_names) if affiliation_names else None\n",
    "    # Fallback: Sometimes the label might be in a <b> tag instead of <h3>\n",
    "    if not affiliations:\n",
    "        label_tag = infobox.find('b', string=re.compile(\"Affiliations:\"))\n",
    "        if label_tag:\n",
    "            value_parts = []\n",
    "            for sibling in label_tag.next_siblings:\n",
    "                if getattr(sibling, 'name', None) == 'b': break\n",
    "                if isinstance(sibling, str):\n",
    "                    cleaned = sibling.strip().replace(':', '').strip()\n",
    "                    if cleaned: value_parts.append(cleaned)\n",
    "            affiliations = \" \".join(value_parts)\n",
    "            \n",
    "    character_data['affiliations'] = affiliations\n",
    "except:\n",
    "    character_data['affiliations'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'infobox' and 'character_data' are already defined in your notebook\n",
    "try:\n",
    "    occupations = None\n",
    "    # Pattern 1: Standard <h3> tag (works for major characters like Kawamatsu)\n",
    "    label_tag = infobox.find('h3', string=re.compile(\"Occupation\"))\n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # Clean out the reference tags (e.g., [1], [3]) first\n",
    "            for sup in value_tag.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            \n",
    "            # Use .stripped_strings to get all pieces of text, including from links\n",
    "            occupation_list = [text.strip().replace(';', '') for text in value_tag.stripped_strings]\n",
    "            \n",
    "            # A small piece of logic to combine \"(former)\" with the preceding occupation\n",
    "            final_list = []\n",
    "            for item in occupation_list:\n",
    "                if item.startswith('(') and final_list:\n",
    "                    final_list[-1] += f\" {item}\"\n",
    "                else:\n",
    "                    final_list.append(item)\n",
    "            \n",
    "            occupations = \", \".join(final_list) if final_list else None\n",
    "    \n",
    "    # Pattern 2 (Fallback): Dense <b> tag (for minor characters)\n",
    "    if not occupations:\n",
    "        label_tag = infobox.find('b', string=re.compile(\"Occupation(s)?:\"))\n",
    "        if label_tag:\n",
    "            value_parts = []\n",
    "            for sibling in label_tag.next_siblings:\n",
    "                if getattr(sibling, 'name', None) == 'b': break # Stop at the next label\n",
    "                if isinstance(sibling, str):\n",
    "                    cleaned = sibling.strip().replace(':', '').strip()\n",
    "                    if cleaned: value_parts.append(cleaned)\n",
    "            occupations = \" \".join(value_parts)\n",
    "\n",
    "    character_data['occupations'] = occupations\n",
    "except:\n",
    "    character_data['occupations'] = None\n",
    "    \n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    origin = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Origin')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not origin:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Origin\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                origin = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['origin'] = origin\n",
    "except:\n",
    "    character_data['origin'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    residence = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Residence')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Residence\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                residence = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['residence'] = residence\n",
    "except:\n",
    "    character_data['residence'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    residence = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Birthday')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Birthday\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                birthday = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['birthday'] = birthday\n",
    "except:\n",
    "    character_data['birthday'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Devil Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    devil_fruit_data = {\n",
    "        'english_name': None,\n",
    "        'japanese_name': None,\n",
    "        'meaning': None,\n",
    "        'type': None\n",
    "    }\n",
    "    \n",
    "   \n",
    "    df_section = infobox.find('h2', class_='pi-header', string='Devil Fruit')\n",
    "\n",
    "    if df_section:\n",
    "        # If the section exists, find each specific field within it\n",
    "        eng_name_tag = df_section.find_next('h3', string='English Name:')\n",
    "        if eng_name_tag:\n",
    "            devil_fruit_data['english_name'] = eng_name_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "        jpn_name_tag = df_section.find_next('h3', string='Japanese Name:')\n",
    "        if jpn_name_tag:\n",
    "            devil_fruit_data['japanese_name'] = jpn_name_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "        meaning_tag = df_section.find_next('h3', string='Meaning:')\n",
    "        if meaning_tag:\n",
    "            devil_fruit_data['meaning'] = meaning_tag.find_next_sibling('div').get_text(strip=True)\n",
    "            \n",
    "        type_tag = df_section.find_next('h3', string='Type:')\n",
    "        if type_tag:\n",
    "            devil_fruit_data['type'] = type_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "    # --- Fallback Method: Look for a single \"Devil Fruit Name\" line ---\n",
    "    # This works for characters like Luffy. We only run this if the primary method found nothing.\n",
    "    if not devil_fruit_data.get('english_name'):\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Devil Fruit Name\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                devil_fruit_data['english_name'] = value_tag.get_text(strip=True)\n",
    "\n",
    "    # Final check: only add the dictionary if we actually found a name.\n",
    "    if devil_fruit_data.get('english_name'):\n",
    "        character_data['devil_fruit'] = devil_fruit_data\n",
    "    else:\n",
    "        character_data['devil_fruit'] = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    character_data['devil_fruit'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Bounty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'infobox' and 'character_data' are already defined in your notebook\n",
    "\n",
    "try:\n",
    "    bounty = None\n",
    "    # FINAL, ROBUST METHOD: Directly find the bounty data container.\n",
    "    bounty_container = infobox.find('div', attrs={'data-source': 'bounty'})\n",
    "    \n",
    "    if bounty_container:\n",
    "        # Get all the text from within the container\n",
    "        full_text = bounty_container.get_text()\n",
    "        \n",
    "        # Use a regular expression to find the first number (with commas)\n",
    "        # This will find \"1,374,000,000\" or \"3,000,000,000\"\n",
    "        match = re.search(r'([\\d,]+)', full_text)\n",
    "        \n",
    "        if match:\n",
    "            # Extract the matched number and remove commas\n",
    "            bounty = match.group(1).replace(',', '')\n",
    "\n",
    "    character_data['bounty'] = bounty\n",
    "except:\n",
    "    character_data['bounty'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    manga_debut, anime_debut = None, None\n",
    "    # Find the <h3> tag whose text contains \"Debut\"\n",
    "    label_tag = infobox.find('h3', string=lambda text: text and \"Debut\" in text.strip())\n",
    "    \n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # First, remove any reference tags (like [1]) to clean the source\n",
    "            for sup in value_tag.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            \n",
    "            # Get the clean text from the container\n",
    "            debut_text = value_tag.get_text(strip=True)\n",
    "            \n",
    "            # Split by either a semicolon or comma to handle variations\n",
    "            parts = re.split(r'[;,]', debut_text)\n",
    "            \n",
    "            for part in parts:\n",
    "                part = part.strip() # Clean up any extra whitespace\n",
    "                if part.startswith(\"Chapter\"):\n",
    "                    manga_debut = part\n",
    "                elif part.startswith(\"Episode\"):\n",
    "                    anime_debut = part\n",
    "                    \n",
    "    character_data['manga_debut'] = manga_debut\n",
    "    character_data['anime_debut'] = anime_debut\n",
    "\n",
    "except:\n",
    "    character_data['manga_debut'] = None\n",
    "    character_data['anime_debut'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    status = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Status')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Status\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                status = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['status'] = status\n",
    "except:\n",
    "    character_data['status'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### test parse infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parse_characters import parse_infobox\n",
    "\n",
    "# get random url from valid_characters\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_data = parse_infobox(url)\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### main content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### General description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content = soup.find('div', class_='mw-parser-output')\n",
    "\n",
    "try:\n",
    "    description = None\n",
    "    \n",
    "    # --- STRATEGY 1: Look for the introductory table (for pages like Bartolomeo) ---\n",
    "    intro_table = main_content.find('table', class_='cs-begin-story')\n",
    "    \n",
    "    if intro_table:\n",
    "        # If the special table exists, get all its text content cleanly.\n",
    "        description = intro_table.get_text(strip=True)\n",
    "    else:\n",
    "        # --- STRATEGY 2 (Fallback): Look for paragraphs before the first major heading ---\n",
    "        # This works for the majority of pages like Luffy and Sabo.\n",
    "        first_heading = main_content.find(re.compile(r'h[2-6]'))\n",
    "        \n",
    "        if first_heading:\n",
    "            intro_paragraphs = []\n",
    "            # Find all <p> tags that appear before the first heading.\n",
    "            for p_tag in first_heading.find_previous_siblings('p'):\n",
    "                # Add a check to ensure we don't accidentally grab a <p> tag that contains the infobox.\n",
    "                if not p_tag.find('aside', class_='portable-infobox'):\n",
    "                    intro_paragraphs.append(p_tag.get_text(strip=True))\n",
    "            \n",
    "            # The results are found in reverse order, so we must reverse the list back.\n",
    "            intro_paragraphs.reverse()\n",
    "            description = \" \".join(intro_paragraphs) if intro_paragraphs else None\n",
    "            \n",
    "    character_data['general_description'] = description\n",
    "\n",
    "except AttributeError:\n",
    "    character_data['general_description'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section_text(section_id):\n",
    "    \"\"\"\n",
    "    A robust function to find a section by its ID and extract all paragraph text\n",
    "    until the next major heading.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        heading = main_content.find('span', id=section_id)\n",
    "        if not heading:\n",
    "            return None # Section does not exist on this page\n",
    "\n",
    "        # Find the parent heading tag (h2, h3, etc.)\n",
    "        parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "        \n",
    "        paragraphs = []\n",
    "        # Iterate through all tags that come AFTER the heading\n",
    "        for sibling in parent_heading.find_next_siblings():\n",
    "            # Stop condition: If we hit the next <h2>, the section is over.\n",
    "            if sibling.name == 'h2':\n",
    "                break\n",
    "            \n",
    "            # If the sibling is a <p> tag, add its text.\n",
    "            if sibling.name == 'p':\n",
    "                paragraphs.append(sibling.get_text(strip=True))\n",
    "        \n",
    "        return \" \".join(paragraphs) if paragraphs else None\n",
    "    except AttributeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_data['appearance'] = extract_section_text(\"Appearance\")\n",
    "\n",
    "character_data['appearance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_data['personality'] = extract_section_text(\"Personality\")\n",
    "character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Now, use the helper to get each section ---\n",
    "\n",
    "character_data['appearance'] = extract_section_text(\"Appearance\")\n",
    "character_data['personality'] = extract_section_text(\"Personality\")\n",
    "character_data['relationships'] = extract_section_text(\"Relationships\")\n",
    "character_data['history'] = extract_section_text(\"History\")\n",
    "\n",
    "print(f\"Appearance: {character_data.get('appearance', '')[:100]}...\")\n",
    "print(f\"Personality: {character_data.get('personality', '')[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
