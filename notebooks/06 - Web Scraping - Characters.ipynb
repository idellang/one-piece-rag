{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# remove column width restrictions\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_path = os.path.join(ROOT, 'data')\n",
    "valid_characters_url = os.path.join(data_path, 'one_piece_characters_urls.txt')\n",
    "\n",
    "# read the file and store each line as an element in a list\n",
    "with open(valid_characters_url, 'r') as file:\n",
    "    valid_characters = [line.strip() for line in file.readlines()]\n",
    "\n",
    "print(f\"Number of valid characters: {len(valid_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# get random url from valid_characters\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "\n",
    "\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 Character Parser - jfcastaneda.led@gmail.com'\n",
    "    }\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "character_data = {'url': url}\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Parse infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = soup.find('aside', class_='portable-infobox')\n",
    "if not infobox:\n",
    "    print(f\"No infobox found for {url}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # The most reliable source is the infobox's main title\n",
    "    character_data['name'] = infobox.find('h2', class_='pi-title').get_text(strip=True)\n",
    "except AttributeError:\n",
    "    character_data['name'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    affiliations = None\n",
    "    \n",
    "    label_tag = infobox.find('h3', string=re.compile(\"Affiliations\"))\n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # Find all the <a> tags, which contain the actual affiliation names\n",
    "            affiliation_links = value_tag.find_all('a')\n",
    "            # Extract the clean text from each link\n",
    "            affiliation_names = [link.get_text(strip=True) for link in affiliation_links]\n",
    "            # Join them into a clean, comma-separated string\n",
    "            affiliations = \", \".join(affiliation_names) if affiliation_names else None\n",
    "    # Fallback: Sometimes the label might be in a <b> tag instead of <h3>\n",
    "    if not affiliations:\n",
    "        label_tag = infobox.find('b', string=re.compile(\"Affiliations:\"))\n",
    "        if label_tag:\n",
    "            value_parts = []\n",
    "            for sibling in label_tag.next_siblings:\n",
    "                if getattr(sibling, 'name', None) == 'b': break\n",
    "                if isinstance(sibling, str):\n",
    "                    cleaned = sibling.strip().replace(':', '').strip()\n",
    "                    if cleaned: value_parts.append(cleaned)\n",
    "            affiliations = \" \".join(value_parts)\n",
    "            \n",
    "    character_data['affiliations'] = affiliations\n",
    "except:\n",
    "    character_data['affiliations'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'infobox' and 'character_data' are already defined in your notebook\n",
    "try:\n",
    "    occupations = None\n",
    "    # Pattern 1: Standard <h3> tag (works for major characters like Kawamatsu)\n",
    "    label_tag = infobox.find('h3', string=re.compile(\"Occupation\"))\n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # Clean out the reference tags (e.g., [1], [3]) first\n",
    "            for sup in value_tag.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            \n",
    "            # Use .stripped_strings to get all pieces of text, including from links\n",
    "            occupation_list = [text.strip().replace(';', '') for text in value_tag.stripped_strings]\n",
    "            \n",
    "            # A small piece of logic to combine \"(former)\" with the preceding occupation\n",
    "            final_list = []\n",
    "            for item in occupation_list:\n",
    "                if item.startswith('(') and final_list:\n",
    "                    final_list[-1] += f\" {item}\"\n",
    "                else:\n",
    "                    final_list.append(item)\n",
    "            \n",
    "            occupations = \", \".join(final_list) if final_list else None\n",
    "    \n",
    "    # Pattern 2 (Fallback): Dense <b> tag (for minor characters)\n",
    "    if not occupations:\n",
    "        label_tag = infobox.find('b', string=re.compile(\"Occupation(s)?:\"))\n",
    "        if label_tag:\n",
    "            value_parts = []\n",
    "            for sibling in label_tag.next_siblings:\n",
    "                if getattr(sibling, 'name', None) == 'b': break # Stop at the next label\n",
    "                if isinstance(sibling, str):\n",
    "                    cleaned = sibling.strip().replace(':', '').strip()\n",
    "                    if cleaned: value_parts.append(cleaned)\n",
    "            occupations = \" \".join(value_parts)\n",
    "\n",
    "    character_data['occupations'] = occupations\n",
    "except:\n",
    "    character_data['occupations'] = None\n",
    "    \n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    origin = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Origin')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not origin:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Origin\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                origin = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['origin'] = origin\n",
    "except:\n",
    "    character_data['origin'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    residence = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Residence')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Residence\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                residence = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['residence'] = residence\n",
    "except:\n",
    "    character_data['residence'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    residence = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Birthday')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Birthday\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                birthday = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['birthday'] = birthday\n",
    "except:\n",
    "    character_data['birthday'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Devil Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    devil_fruit_data = {\n",
    "        'english_name': None,\n",
    "        'japanese_name': None,\n",
    "        'meaning': None,\n",
    "        'type': None\n",
    "    }\n",
    "    \n",
    "   \n",
    "    df_section = infobox.find('h2', class_='pi-header', string='Devil Fruit')\n",
    "\n",
    "    if df_section:\n",
    "        # If the section exists, find each specific field within it\n",
    "        eng_name_tag = df_section.find_next('h3', string='English Name:')\n",
    "        if eng_name_tag:\n",
    "            devil_fruit_data['english_name'] = eng_name_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "        jpn_name_tag = df_section.find_next('h3', string='Japanese Name:')\n",
    "        if jpn_name_tag:\n",
    "            devil_fruit_data['japanese_name'] = jpn_name_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "        meaning_tag = df_section.find_next('h3', string='Meaning:')\n",
    "        if meaning_tag:\n",
    "            devil_fruit_data['meaning'] = meaning_tag.find_next_sibling('div').get_text(strip=True)\n",
    "            \n",
    "        type_tag = df_section.find_next('h3', string='Type:')\n",
    "        if type_tag:\n",
    "            devil_fruit_data['type'] = type_tag.find_next_sibling('div').get_text(strip=True)\n",
    "\n",
    "    # --- Fallback Method: Look for a single \"Devil Fruit Name\" line ---\n",
    "    # This works for characters like Luffy. We only run this if the primary method found nothing.\n",
    "    if not devil_fruit_data.get('english_name'):\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Devil Fruit Name\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                devil_fruit_data['english_name'] = value_tag.get_text(strip=True)\n",
    "\n",
    "    # Final check: only add the dictionary if we actually found a name.\n",
    "    if devil_fruit_data.get('english_name'):\n",
    "        character_data['devil_fruit'] = devil_fruit_data\n",
    "    else:\n",
    "        character_data['devil_fruit'] = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    character_data['devil_fruit'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Bounty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'infobox' and 'character_data' are already defined in your notebook\n",
    "\n",
    "try:\n",
    "    bounty = None\n",
    "    # FINAL, ROBUST METHOD: Directly find the bounty data container.\n",
    "    bounty_container = infobox.find('div', attrs={'data-source': 'bounty'})\n",
    "    \n",
    "    if bounty_container:\n",
    "        # Get all the text from within the container\n",
    "        full_text = bounty_container.get_text()\n",
    "        \n",
    "        # Use a regular expression to find the first number (with commas)\n",
    "        # This will find \"1,374,000,000\" or \"3,000,000,000\"\n",
    "        match = re.search(r'([\\d,]+)', full_text)\n",
    "        \n",
    "        if match:\n",
    "            # Extract the matched number and remove commas\n",
    "            bounty = match.group(1).replace(',', '')\n",
    "\n",
    "    character_data['bounty'] = bounty\n",
    "except:\n",
    "    character_data['bounty'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    manga_debut, anime_debut = None, None\n",
    "    # Find the <h3> tag whose text contains \"Debut\"\n",
    "    label_tag = infobox.find('h3', string=lambda text: text and \"Debut\" in text.strip())\n",
    "    \n",
    "    if label_tag:\n",
    "        value_tag = label_tag.find_next_sibling('div')\n",
    "        if value_tag:\n",
    "            # First, remove any reference tags (like [1]) to clean the source\n",
    "            for sup in value_tag.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            \n",
    "            # Get the clean text from the container\n",
    "            debut_text = value_tag.get_text(strip=True)\n",
    "            \n",
    "            # Split by either a semicolon or comma to handle variations\n",
    "            parts = re.split(r'[;,]', debut_text)\n",
    "            \n",
    "            for part in parts:\n",
    "                part = part.strip() # Clean up any extra whitespace\n",
    "                if part.startswith(\"Chapter\"):\n",
    "                    manga_debut = part\n",
    "                elif part.startswith(\"Episode\"):\n",
    "                    anime_debut = part\n",
    "                    \n",
    "    character_data['manga_debut'] = manga_debut\n",
    "    character_data['anime_debut'] = anime_debut\n",
    "\n",
    "except:\n",
    "    character_data['manga_debut'] = None\n",
    "    character_data['anime_debut'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    status = None\n",
    "    # Pattern 1: Dedicated Origin section\n",
    "    origin_section = infobox.find('h2', string='Status')\n",
    "    if origin_section:\n",
    "        origin_div = origin_section.find_next('div', class_='pi-data-value')\n",
    "        if origin_div:\n",
    "            origin = origin_div.get_text(strip=True)\n",
    "\n",
    "    # Pattern 2 (Fallback): Standard label\n",
    "    if not residence:\n",
    "        label_tag = infobox.find('h3', string=re.compile(\"Status\"))\n",
    "        if label_tag:\n",
    "            value_tag = label_tag.find_next_sibling('div')\n",
    "            if value_tag:\n",
    "                status = value_tag.get_text(strip=True)\n",
    "\n",
    "    character_data['status'] = status\n",
    "except:\n",
    "    character_data['status'] = None\n",
    "\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### test parse infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parse_characters import parse_infobox\n",
    "\n",
    "# get random url from valid_characters\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 Character Parser - jfcastaneda.led@gmail.com'\n",
    "    }\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_data = parse_infobox(soup)\n",
    "character_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### main content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 Character Parser - jfcastaneda.led@gmail.com'\n",
    "    }\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content = soup.find('div', class_='mw-parser-output')\n",
    "if not main_content:\n",
    "    print(f\"No main content found for {url}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### General info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_main_data = {'url': url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info_texts = []\n",
    "\n",
    "for element in main_content.find_all(recursive=False):\n",
    "    if element.name == 'h2':\n",
    "        break  # Stop if we reach the next main section\n",
    "\n",
    "    if element.name in ['p']:\n",
    "        for sup in element.find_all('sup'):\n",
    "            sup.decompose()\n",
    "        general_info_texts.append(element.get_text(strip=True))\n",
    "    \n",
    "general_info =  \" \".join(general_info_texts) if general_info_texts else None\n",
    "character_main_data['general_info'] = general_info\n",
    "character_main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_section(main_content, section_id_pattern):\n",
    "    \"\"\"\n",
    "    A generic function to parse a specific section from the main content area.\n",
    "\n",
    "    It finds a section header based on a pattern and extracts text from all\n",
    "    subsequent <p> and <ul> tags until the next main header (<h2>) is found.\n",
    "\n",
    "    Args:\n",
    "        main_content (bs4.element.Tag): The BeautifulSoup tag for the main content area.\n",
    "        section_id_pattern (str or re.Pattern): The ID to find in the section's span tag.\n",
    "                                                 Can be a string or a compiled regex.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all the text from the section, or None.\n",
    "    \"\"\"\n",
    "    # Find the header span tag using the provided ID or regex pattern\n",
    "    section_header = main_content.find('span', id=section_id_pattern)\n",
    "    if not section_header:\n",
    "        return None\n",
    "\n",
    "    section_texts = []\n",
    "    # Start iterating from the header's parent (the <h2> tag)\n",
    "    element = section_header.find_parent('h2')\n",
    "\n",
    "    # Loop through all the tags that come after the header\n",
    "    for sibling in element.find_next_siblings():\n",
    "        # The next <h2> tag marks the end of our current section\n",
    "        if sibling.name == 'h2':\n",
    "            break\n",
    "\n",
    "        # We are interested in paragraphs (<p>) and unordered lists (<ul>)\n",
    "        if sibling.name == 'p':\n",
    "            # Clean out reference tags (e.g., [1], [3]) before getting text\n",
    "            for sup in sibling.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            section_texts.append(sibling.get_text(strip=True))\n",
    "        elif sibling.name == 'ul':\n",
    "            # For lists, get the text from each list item (<li>)\n",
    "            for li in sibling.find_all('li'):\n",
    "                for sup in li.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "                section_texts.append(li.get_text(strip=True))\n",
    "\n",
    "    return \" \".join(section_texts) if section_texts else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance = parse_section(main_content, section_id_pattern = \"Appearance\")\n",
    "personality = parse_section(main_content, section_id_pattern = \"Personality\")\n",
    "history = parse_section(main_content, section_id_pattern = \"History\")\n",
    "abilities = parse_section(main_content,  section_id_pattern=re.compile(r'^Abilities_and'))\n",
    "relationships = parse_section(main_content, section_id_pattern= \"Relationships\")\n",
    "\n",
    "character_main_data['appearance'] = appearance\n",
    "character_main_data['personality'] = personality\n",
    "character_main_data['relations'] = relationships\n",
    "character_main_data['history'] = history\n",
    "character_main_data['abilities'] = abilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_header = main_content.find('span', id='Trivia')\n",
    "if trivia_header:\n",
    "    trivia_texts = []\n",
    "    # 2. Start iterating from the header's parent (the <h2> tag)\n",
    "    element = trivia_header.find_parent('h2')\n",
    "    # 3. Loop through all the tags that come after the header\n",
    "    for sibling in element.find_next_siblings():\n",
    "        # The next <h2> tag marks the end of the trivia section\n",
    "        if sibling.name == 'h2':\n",
    "            break\n",
    "        # 4. Specifically look for unordered lists (<ul>)\n",
    "        if sibling.name == 'ul':\n",
    "            # 5. Find all list items (<li>) within the list\n",
    "            for li in sibling.find_all('li'):\n",
    "                # Clean out reference tags (e.g., [81])\n",
    "                for sup in li.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "                trivia_texts.append(li.get_text(strip=True))\n",
    "    trivia = \" \".join(trivia_texts) if trivia_texts else None\n",
    "else:\n",
    "    trivia = None\n",
    "\n",
    "character_main_data['trivia'] = trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Combine both into functions and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parse_characters import parse_infobox, parse_main_content\n",
    "\n",
    "# get random url from valid_characters\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")\n",
    "\n",
    "character_main_data = {'url': url}\n",
    "\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 Character Parser - jfcastaneda.led@gmail.com'\n",
    "    }\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_data = parse_infobox(soup)\n",
    "character_main_data.update(infobox_data)\n",
    "character_main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content_data = parse_main_content(soup)\n",
    "character_main_data.update(main_content_data)\n",
    "character_main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parse_characters import parse_character\n",
    "\n",
    "url = random.choice(valid_characters)\n",
    "print(f\"Random URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Parse characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "JSON_FILE = os.path.join(data_path, 'one_piece_characters_data.jsonl')\n",
    "\n",
    "if os.path.exists(JSON_FILE):\n",
    "    all_characters = []\n",
    "    with open(JSON_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    all_characters.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line: {line[:100]}... Error: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    scraped_characters = {char['url'] for char in all_characters if 'url' in char}\n",
    "    print(f\"Already scraped {len(scraped_characters)} characters.\")\n",
    "else:\n",
    "    all_characters = []\n",
    "    scraped_characters = set()\n",
    "    print(\"Starting fresh scrape.\")\n",
    "\n",
    "# Get characters that haven't been scraped yet\n",
    "characters_to_scrape = [url for url in valid_characters if url not in scraped_characters]\n",
    "print(f\"Characters left to scrape: {len(characters_to_scrape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parse_characters import parse_character\n",
    "\n",
    "scraper_headers = {\n",
    "    'User-Agent': 'OnePieceRAGBot/1.0 (Learning Project; contact: jfcastaneda.led@gmail.com)'\n",
    "}\n",
    "\n",
    "errors = []\n",
    "ERROR_FILE = os.path.join(data_path, 'character_scraping_errors.jsonl')\n",
    "\n",
    "print(f\"Starting to parse {len(characters_to_scrape)} remaining characters...\")\n",
    "for character_url in tqdm(characters_to_scrape):\n",
    "    try:\n",
    "        # Check if already scraped (extra safety check)\n",
    "        if character_url in scraped_characters:\n",
    "            print(f\"Skipping already scraped: {character_url}\")\n",
    "            continue\n",
    "            \n",
    "        character_data = parse_character(character_url)\n",
    "\n",
    "        # Check if parsing was successful\n",
    "        if 'error' in character_data:\n",
    "            print(f\"❌ Failed to parse {character_url}: {character_data.get('error')}\")\n",
    "            errors.append((character_url, character_data.get('error', 'Unknown error')))\n",
    "            \n",
    "            # Save error data to separate file\n",
    "            error_data = {\n",
    "                'url': character_url,\n",
    "                'error': character_data.get('error', 'Unknown error'),\n",
    "                'error_type': 'parsing_failed',\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            with open(ERROR_FILE, 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(error_data, ensure_ascii=False) + '\\n')\n",
    "        else:\n",
    "            print(f\"✅ Successfully parsed: {character_data.get('name', 'Unknown')} - {character_url}\")\n",
    "            \n",
    "            # Add to scraped set to avoid duplicates\n",
    "            scraped_characters.add(character_url)\n",
    "            \n",
    "            # Save ONLY successful character data\n",
    "            with open(os.path.join(data_path, 'one_piece_characters_data.jsonl'), 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(character_data, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception parsing {character_url}: {e}\")\n",
    "        errors.append((character_url, str(e)))\n",
    "        \n",
    "        # Save exception data to separate error file\n",
    "        error_data = {\n",
    "            'url': character_url,\n",
    "            'error': f'Exception: {str(e)}',\n",
    "            'error_type': 'exception',\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        with open(ERROR_FILE, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(error_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    # Be respectful to the server\n",
    "    time.sleep(0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
