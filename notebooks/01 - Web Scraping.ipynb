{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- Scrape one piece chapter wiki\n",
    "- Store it in a json structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://onepiece.fandom.com/wiki/Chapter_1\"\n",
    "\n",
    "print(f\"Scraping {url}...\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully fetched the page.\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        print(\"HTML content:\")\n",
    "        print(soup.prettify()) \n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Use infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        print(\"Successfully fetched the page.\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        infobox = soup.find('aside', class_='portable-infobox')\n",
    "\n",
    "        main_content = soup.find('div', class_='mw-parser-output')\n",
    "\n",
    "        print(\"Infobox content:\")\n",
    "        if infobox:\n",
    "            print(infobox.prettify())\n",
    "        else:\n",
    "            print(\"No infobox found.\")\n",
    "\n",
    "        print(\"\\nMain content:\")\n",
    "        if main_content:\n",
    "            print(main_content.prettify())\n",
    "        else:\n",
    "            print(\"No main content found.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Extract key fields\n",
    "\n",
    "#### From infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_data = {}\n",
    "chapter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dateutil.parser import parse \n",
    "\n",
    "# chapter title\n",
    "chapter_data['chapter_title'] = infobox.find('h2', class_='pi-title').get_text(strip=True) \n",
    "\n",
    "# find Div with \"Chapter\" and assign its sibling div\n",
    "chapter_num_div = infobox.find('h3', string = re.compile(\"Chapter\")).find_next_sibling('div')\n",
    "\n",
    "if chapter_num_div:\n",
    "    chapter_data['chapter_number'] = int(chapter_num_div.get_text(strip=True))\n",
    "\n",
    "# release date\n",
    "# Find <H3> with \"Release date\" and get its sibling <div>\n",
    "release_date_div = infobox.find('h3', string=re.compile(\"Release Date:\")).find_next_sibling('div')\n",
    "# extract release date with date format\n",
    "if release_date_div:\n",
    "    # get raw text\n",
    "    raw_date_str = release_date_div.get_text(strip=True)\n",
    "    try:\n",
    "        date_obj = parse(raw_date_str, fuzzy=True)\n",
    "        chapter_data['release_date'] = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        print(f\"Could not parse date from string: {raw_date_str}\")\n",
    "        chapter_data['release_date'] = raw_date_str  # fallback to raw string\n",
    "\n",
    "chapter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### From main content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content = soup.find('div', class_='mw-parser-output')\n",
    "if main_content:\n",
    "    # Short summary\n",
    "    # Find H2 with a child that has id \"Short_summary\"\n",
    "    short_summary_heading = main_content.find('span', id='Short_Summary')\n",
    "    if short_summary_heading:\n",
    "        # Get the next sibling <p> tag\n",
    "        short_summary_paragraph = short_summary_heading.find_parent('h2').find_next_sibling('p')\n",
    "        if short_summary_paragraph:\n",
    "            chapter_data['short_summary'] = short_summary_paragraph.get_text(strip=True)\n",
    "\n",
    "    long_summary_heading = main_content.find('span', id='Long_Summary')\n",
    "    if long_summary_heading:\n",
    "        # initialize list to hold paragraphs\n",
    "        long_summary_text = []\n",
    "        for p_tag in long_summary_heading.find_parent('h2').find_next_siblings('p'):\n",
    "            if p_tag.get_text(strip=True):\n",
    "                long_summary_text.append(p_tag.get_text(strip=True))\n",
    "            else:\n",
    "                # stop when we hit a non-paragraph tag or empty paragraph\n",
    "                break\n",
    "        chapter_data['long_summary'] = \" \".join(long_summary_text)\n",
    "\n",
    "chapter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print short summary\n",
    "print(\"Short Summary:\")\n",
    "print(chapter_data.get('short_summary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print long summary\n",
    "print(\"\\nLong Summary:\")\n",
    "print(chapter_data.get('long_summary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    notes_heading = main_content.find('span', id='Chapter_Notes')\n",
    "    if notes_heading:\n",
    "        # Find the <ul> that follows the heading\n",
    "        ul_tag = notes_heading.find_parent('h3').find_next_sibling('ul')\n",
    "        if ul_tag:\n",
    "            # extract list items\n",
    "            notes = [li.get_text(strip=True) for li in ul_tag.find_all('li')]\n",
    "            chapter_data['notes'] = \"\\n\".join(notes)\n",
    "    else:\n",
    "        chapter_data['notes'] = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while extracting notes: {e}\")\n",
    "    chapter_data['notes'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chars_heading = main_content.find('span', id='Characters')\n",
    "\n",
    "    if chars_heading:\n",
    "        table_tag = chars_heading.find_parent('h3').find_next_sibling('table', class_='CharTable')\n",
    "        \n",
    "        if table_tag:\n",
    "            character_subgroups = {}\n",
    "\n",
    "            rows = table_tag.find('tbody').find_all('tr')\n",
    "\n",
    "            if len(rows) >= 2: # Check for at least 2 rows (header and data)\n",
    "                # Get headers from the first row\n",
    "                headers = [th.get_text(strip=True) for th in rows[0].find_all('th')]\n",
    "\n",
    "                # Get character data from the second row's td tags\n",
    "                data_cells = rows[1].find_all('td')\n",
    "\n",
    "                # Pair each header with its corresponding data cell\n",
    "                for i, header in enumerate(headers):\n",
    "                    if i < len(data_cells):\n",
    "                        characters = [li.get_text(strip=True) for li in data_cells[i].find_all('li')]\n",
    "                        character_subgroups[header] = characters\n",
    "            \n",
    "            chapter_data['characters'] = character_subgroups\n",
    "    else:\n",
    "        chapter_data['characters'] = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while extracting characters: {e}\")\n",
    "    chapter_data['characters'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Triva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trivia_heading = main_content.find('span', id='Trivia')\n",
    "    if trivia_heading:\n",
    "        ul_tag = trivia_heading.find_parent('h2').find_next_sibling('ul')\n",
    "        if ul_tag:\n",
    "            trivia = [li.get_text(strip=True) for li in ul_tag.find_all('li')]\n",
    "            chapter_data['trivia'] = \"\\n\".join(trivia)\n",
    "    else:\n",
    "        chapter_data['trivia'] = None\n",
    "except AttributeError:\n",
    "    chapter_data['trivia'] = None\n",
    "\n",
    "chapter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Move parsing into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chapter(url, headers = None):\n",
    "    \"\"\"\n",
    "    Fetches and parses single chapter page from One Piece Fandom wiki.\n",
    "    Returns dictionary of chapter data\n",
    "    Missing fields are set to None\n",
    "    \"\"\"\n",
    "\n",
    "    # SAFEGUARD: Handle network errors and bad HTTP responses upfront.\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred fetching {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # if request is successful, parse the content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    chapter_data = {\n",
    "        'url': url,\n",
    "    }\n",
    "    \n",
    "    # Extract from infobox\n",
    "    infobox = soup.find('aside', class_='portable-infobox')\n",
    "\n",
    "    if infobox:\n",
    "        print(\"Extracting infobox data...\")\n",
    "        try:\n",
    "            chapter_data['chapter_title'] = infobox.find('h2', class_='pi-title').get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            chapter_data['chapter_title'] = None\n",
    "\n",
    "        try:\n",
    "            chapter_num_div = infobox.find('h3', string = re.compile(\"Chapter\")).find_next_sibling('div')\n",
    "            chapter_data['chapter_number'] = int(chapter_num_div.get_text(strip=True)) if chapter_num_div else None\n",
    "        except (AttributeError, ValueError):\n",
    "            chapter_data['chapter_number'] = None\n",
    "        \n",
    "        try:\n",
    "            release_date_div = infobox.find('h3', string=re.compile(\"Release Date:\")).find_next_sibling('div')\n",
    "            if release_date_div:\n",
    "                raw_date_str = release_date_div.get_text(strip=True)\n",
    "                date_obj = parse(raw_date_str, fuzzy=True)\n",
    "                chapter_data['release_date'] = date_obj.strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                chapter_data['release_date'] = None\n",
    "        except (AttributeError, ValueError):\n",
    "            chapter_data['release_date'] = None\n",
    "\n",
    "    else:\n",
    "        print(\"No infobox found.\")\n",
    "        chapter_data['chapter_title'] = None\n",
    "        chapter_data['chapter_number'] = None\n",
    "        chapter_data['release_date'] = None\n",
    "\n",
    "    main_content = soup.find('div', class_='mw-parser-output')\n",
    "\n",
    "    if main_content:\n",
    "        print(\"Extracting main content data...\")\n",
    "        # Short summary\n",
    "        try:\n",
    "            heading = main_content.find('span', id='Short_Summary')\n",
    "            if heading:\n",
    "                # Use a loop to find all subsequent <p> tags until the next heading\n",
    "                summary_ps = []\n",
    "                for sibling in heading.find_parent('h2').find_next_siblings():\n",
    "                    if sibling.name == 'p':\n",
    "                        summary_ps.append(sibling.get_text(strip=True))\n",
    "                    else:\n",
    "                        # Stop when we hit a non-paragraph tag (like the next <h2>)\n",
    "                        break\n",
    "                chapter_data['short_summary'] = \" \".join(summary_ps) if summary_ps else None\n",
    "            else:\n",
    "                chapter_data['short_summary'] = None\n",
    "        except AttributeError:\n",
    "            chapter_data['short_summary'] = None\n",
    "\n",
    "        # Long Summary\n",
    "        try:\n",
    "            heading = main_content.find('span', id='Long_Summary')\n",
    "            if heading:\n",
    "                summary_ps = []\n",
    "                for sibling in heading.find_parent('h2').find_next_siblings():\n",
    "                    if sibling.name == 'p':\n",
    "                        summary_ps.append(sibling.get_text(strip=True))\n",
    "                    else:\n",
    "                        break\n",
    "                chapter_data['long_summary'] = \" \".join(summary_ps) if summary_ps else None\n",
    "            else:\n",
    "                chapter_data['long_summary'] = None\n",
    "        except AttributeError:\n",
    "            chapter_data['long_summary'] = None\n",
    "\n",
    "        # Chapter notes\n",
    "        try:\n",
    "            heading = main_content.find('span', id='Chapter_Notes')\n",
    "            if heading:\n",
    "                ul = heading.find_parent('h3').find_next_sibling('ul')\n",
    "                notes = [li.get_text(strip=True) for li in ul.find_all('li', recursive=False)]\n",
    "                chapter_data['chapter_notes'] = \"\\n\".join(notes)\n",
    "            else:\n",
    "                chapter_data['chapter_notes'] = None\n",
    "        except AttributeError:\n",
    "            chapter_data['chapter_notes'] = None\n",
    "        \n",
    "        # Characters\n",
    "        try:\n",
    "            chars_heading = main_content.find('span', id='Characters')\n",
    "            if chars_heading:\n",
    "                table_tag = chars_heading.find_parent('h3').find_next_sibling('table', class_='CharTable')\n",
    "                if table_tag:\n",
    "                    character_groups = {}\n",
    "                    rows = table_tag.find('tbody').find_all('tr')\n",
    "\n",
    "                    if len(rows) >= 2:\n",
    "                        headers = [th.get_text(strip=True) for th in rows[0].find_all('th')]\n",
    "                        data_cells = rows[1].find_all('td')\n",
    "\n",
    "                        for i, header in enumerate(headers):\n",
    "                            if i < len(data_cells):\n",
    "                                cell = data_cells[i]\n",
    "                                subgroups_in_cell = {}\n",
    "                                \n",
    "                                # Find all <dl> tags, which define the subgroups.\n",
    "                                subgroup_dls = cell.find_all('dl')\n",
    "\n",
    "                                if subgroup_dls:\n",
    "                                    for dl in subgroup_dls:\n",
    "                                        dt = dl.find('dt')\n",
    "                                        if not dt: continue # Skip if a <dl> has no <dt> title\n",
    "                                        \n",
    "                                        subgroup_title = dt.get_text(strip=True)\n",
    "\n",
    "                                        # STRATEGY:\n",
    "                                        # First, look for a <ul> INSIDE the <dl> (Pattern A)\n",
    "                                        character_ul = dl.find('ul')\n",
    "                                        \n",
    "                                        # If not found, look for a <ul> as the NEXT SIBLING of the <dl> (Pattern B)\n",
    "                                        if not character_ul:\n",
    "                                            character_ul = dl.find_next_sibling('ul')\n",
    "                                        \n",
    "                                        if character_ul:\n",
    "                                            characters = [li.get_text(strip=True) for li in character_ul.find_all('li')]\n",
    "                                            subgroups_in_cell[subgroup_title] = characters\n",
    "                                else:\n",
    "                                    # Fallback for simple tables with no <dl> subgroups at all.\n",
    "                                    characters = [li.get_text(strip=True) for li in cell.find_all('li')]\n",
    "                                    if characters:\n",
    "                                        subgroups_in_cell[header] = characters\n",
    "\n",
    "                                character_groups[header] = subgroups_in_cell\n",
    "                    \n",
    "                    chapter_data['characters'] = character_groups\n",
    "            else:\n",
    "                chapter_data['characters'] = None\n",
    "        except (AttributeError, IndexError) as e:\n",
    "            print(f\"An error occurred while extracting characters: {e}\")\n",
    "            chapter_data['characters'] = None\n",
    "            \n",
    "        # Trivia\n",
    "        try:\n",
    "            heading = main_content.find('span', id='Trivia')\n",
    "            if heading:\n",
    "                ul = heading.find_parent('h2').find_next_sibling('ul')\n",
    "                trivia = [li.get_text(strip=True) for li in ul.find_all('li', recursive=False)]\n",
    "                chapter_data['trivia'] = \"\\n\".join(trivia)\n",
    "            else:\n",
    "                chapter_data['trivia'] = None\n",
    "        except AttributeError:\n",
    "            chapter_data['trivia'] = None\n",
    "        \n",
    "    else:\n",
    "        print(\"No main content found.\")\n",
    "        chapter_data['short_summary'] = None\n",
    "        chapter_data['long_summary'] = None\n",
    "        chapter_data['notes'] = None\n",
    "        chapter_data['characters'] = None\n",
    "        chapter_data['trivia'] = None\n",
    "        return chapter_data\n",
    "    \n",
    "\n",
    "\n",
    "    return chapter_data\n",
    "\n",
    "chapter_data = parse_chapter(url)\n",
    "chapter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Test run on 10 chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "CHAPTER_NUMBERS_TO_TEST = np.random.choice(range(1, 1156), size=10, replace=False)\n",
    "BASE_URL = \"https://onepiece.fandom.com/wiki/\"\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 (Learning Project; contact: jfcastaneda.led@gmail.com)'\n",
    "    }\n",
    "\n",
    "test_urls = [f\"{BASE_URL}Chapter_{num}\" for num in CHAPTER_NUMBERS_TO_TEST]\n",
    "test_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "all_chapters_data = []\n",
    "\n",
    "for url in tqdm(test_urls, desc=\"Scraping chapters\"):\n",
    "    chapter_data = parse_chapter(url, headers=scraper_headers)\n",
    "\n",
    "    # append only if successful\n",
    "    if chapter_data:\n",
    "        all_chapters_data.append(chapter_data)\n",
    "\n",
    "    time.sleep(0.5)  # be polite and avoid overwhelming the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "ROOT = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '.'))\n",
    "DATA_PATH = os.path.join(ROOT, 'data')\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'one_piece_chapters.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chapters_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Do a FULL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE = os.path.join(DATA_PATH, 'one_piece_chapters.json')\n",
    "ALL_CHAPTERS = range(0, 1156)\n",
    "\n",
    "# load existing data to avoid re-scraping\n",
    "if os.path.exists(JSON_FILE):\n",
    "    with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        all_chapters_data = json.load(f)\n",
    "    scraped_urls = {entry['url'] for entry in all_chapters_data if 'url' in entry}\n",
    "    print(f\"Loaded {len(all_chapters_data)} existing entries from {JSON_FILE}.\")\n",
    "else:\n",
    "    all_chapters_data = []\n",
    "    scraped_urls = set()\n",
    "    print(f\"No existing data found. Starting fresh.\")\n",
    "\n",
    "\n",
    "# Create set of chapters scraped so far\n",
    "scraped_chapter_numbers = {ch.get('chapter_number') for ch in all_chapters_data}\n",
    "print(f\"Already scraped chapter numbers: {sorted(scraped_chapter_numbers)}\")\n",
    "\n",
    "chapters_to_scrape = [num for num in ALL_CHAPTERS if num not in scraped_chapter_numbers]\n",
    "print(f\"Chapters left to scrape: {len(chapters_to_scrape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_chapters_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting full scrape...\")\n",
    "BASE_URL = \"https://onepiece.fandom.com/wiki/\"\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 (Learning Project; contact: jfcastaneda.led@gmail.com)'\n",
    "}\n",
    "\n",
    "for chapter_num in tqdm(chapters_to_scrape, desc=\"Scraping chapters\"):\n",
    "    url = f\"{BASE_URL}Chapter_{chapter_num}\"\n",
    "    \n",
    "    chapter_data = parse_chapter(url, headers=scraper_headers)\n",
    "\n",
    "    if chapter_data and chapter_data.get('chapter_number') is not None:\n",
    "        all_chapters_data.append(chapter_data)\n",
    "\n",
    "        # Save progress after each successful scrape\n",
    "        with open(JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chapters_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    time.sleep(0.5)  # be polite and avoid overwhelming the server\n",
    "    \n",
    "print(f\"\\nScraping complete. Total chapters in file: {len(all_chapters_data)}.\")\n",
    "print(f\"Data saved to {JSON_FILE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
