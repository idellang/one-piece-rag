{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### To parse\n",
    "\n",
    "- url\n",
    "- episode number\n",
    "- episode title (Under japanese information)\n",
    "- air date (Under japanese information)\n",
    "- source chapters (unders statistics)\n",
    "- short summary\n",
    "- long summary\n",
    "- characters in order of appearance\n",
    "- anime notes\n",
    "- Trivia (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Request from one piece wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "MAX_EP = 1142\n",
    "\n",
    "# randomize from 1 to MAX_EP\n",
    "ep_num = random.randint(1, MAX_EP)\n",
    "ep_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://onepiece.fandom.com/wiki/Episode_{ep_num}'\n",
    "scraper_headers = {\n",
    "        'User-Agent': 'OnePieceRAGBot/1.0 (Learning Project; contact: jfcastaneda.led@gmail.com)'\n",
    "    }\n",
    "\n",
    "# fetch page\n",
    "response = requests.get(url, headers=scraper_headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data = {'url': url}\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Parse infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = soup.find('aside', class_='portable-infobox')\n",
    "if not infobox:\n",
    "    print(\"No infobox found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Num div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode number\n",
    "# fallback because the num div is not within Episode #\n",
    "\n",
    "num_div_standard = infobox.find('div', string = 'Episode #')\n",
    "\n",
    "if num_div_standard:\n",
    "    value_div = num_div_standard.find_next_sibling('div')\n",
    "    if value_div:\n",
    "        episode_number = int(value_div.get_text(strip=True))\n",
    "else:\n",
    "    print(\"Standard method failed, trying fallback.\")\n",
    "    nav_tag = infobox.find('nav', class_ = 'pi-navigation')\n",
    "    if nav_tag:\n",
    "        span_tag = nav_tag.find('span', class_ = 'nomobile')\n",
    "        # extract digits from text\n",
    "        text = span_tag.get_text(strip=True)\n",
    "        num_match = re.search(r'\\d+', text)\n",
    "        if num_match:\n",
    "            episode_number = int(num_match.group())\n",
    "\n",
    "episode_data['episode_number'] = episode_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Episode title: English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title = infobox.find('h2', class_ = 'pi-title').get_text(strip= True, separator= ' ')\n",
    "    episode_data['english_title'] = title\n",
    "except AttributeError:\n",
    "    print(\"No title found in the infobox.\")\n",
    "    episode_data['english_title'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Episode title: Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    romaji_div = infobox.find('h3', string = 'Romaji').find_next_sibling('div')\n",
    "    episode_data['romaji_title'] = romaji_div.get_text(strip= True, separator= ' ')\n",
    "except AttributeError:\n",
    "    print(\"No Romaji title found in the infobox.\")\n",
    "    episode_data['romaji_title'] = None\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_div = infobox.find('h3', string = 'Airdate').find_next_sibling('div')\n",
    "raw_date = date_div.get_text(strip= True).split('[')[0]\n",
    "episode_data['air_date'] = parse(raw_date).strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    date_div = infobox.find('h3', string = 'Airdate').find_next_sibling('div')\n",
    "    raw_date = date_div.get_text(strip= True).split('[')[0]\n",
    "    episode_data['air_date'] = parse(raw_date).strftime('%Y-%m-%d')\n",
    "except(AttributeError, ValueError, TypeError):\n",
    "    episode_data['air_date'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Source chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    source_div = infobox.find('h3', string = 'Chapters').find_next_sibling('div')\n",
    "    # use separator to handle multiple lines\n",
    "    episode_data['source_chapters'] = source_div.get_text(strip= True, separator= ', ')\n",
    "except AttributeError:\n",
    "    episode_data['source_chapters'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Staff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_data = {}\n",
    "try:\n",
    "    # Find the parent section for all credits\n",
    "    credits_section = infobox.find('h2', string='Episode Credits').find_parent('section')\n",
    "    \n",
    "    # Extract each staff member\n",
    "    screenplay_div = credits_section.find('h3', string='Screenplay').find_next_sibling('div')\n",
    "    staff_data['screenplay'] = screenplay_div.get_text(strip=True) if screenplay_div else None\n",
    "    \n",
    "    art_div = credits_section.find('h3', string='Art').find_next_sibling('div')\n",
    "    staff_data['art'] = art_div.get_text(strip=True) if art_div else None\n",
    "    \n",
    "    animation_div = credits_section.find('h3', string='Animation').find_next_sibling('div')\n",
    "    staff_data['animation'] = animation_div.get_text(strip=True) if animation_div else None\n",
    "    \n",
    "    direction_div = credits_section.find('h3', string='Direction').find_next_sibling('div')\n",
    "    staff_data['direction'] = direction_div.get_text(strip=True) if direction_div else None\n",
    "\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "episode_data['staff'] = staff_data\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Main content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content = soup.find('div', class_ = 'mw-parser-output')\n",
    "main_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    short_summary = None\n",
    "    heading = main_content.find('span', id='Short_Summary')\n",
    "    if heading:\n",
    "        summary_ps = []\n",
    "        parent_heading = heading.find_parent('h2')\n",
    "                \n",
    "        for sibling in parent_heading.find_next_siblings():\n",
    "            if sibling.name == 'h2':\n",
    "                break\n",
    "                    \n",
    "                    # Only collect text from paragraph tags\n",
    "            if sibling.name == 'p':\n",
    "                    summary_ps.append(sibling.get_text(strip=True))\n",
    "                        \n",
    "            short_summary = \" \".join(summary_ps) if summary_ps else None\n",
    "                \n",
    "    episode_data['short_summary'] = short_summary\n",
    "            \n",
    "except AttributeError:\n",
    "    episode_data['short_summary'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Long Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    long_summary = None\n",
    "    heading = main_content.find('span', id = 'Long_Summary')\n",
    "    if heading:\n",
    "        parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "\n",
    "        summary_ps = []\n",
    "\n",
    "        for sibling in parent_heading.find_next_siblings():\n",
    "            if sibling.name == 'h2': # Stop condition remains the same\n",
    "                break\n",
    "            if sibling.name == 'p':\n",
    "                summary_ps.append(sibling.get_text(strip=True))\n",
    "\n",
    "        \n",
    "        long_summary = ' '.join(summary_ps) if summary_ps else None\n",
    "\n",
    "    episode_data['long_summary'] = long_summary\n",
    "except AttributeError:\n",
    "    episode_data['long_summary'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    characters = None\n",
    "    # Find the heading for the characters section\n",
    "    heading = main_content.find('span', id='Characters_in_Order_of_Appearance')\n",
    "    \n",
    "    if heading:\n",
    "        # Robustly find the parent heading tag (h2, h3, etc.)\n",
    "        parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "        \n",
    "        # Find the very next element that follows the heading\n",
    "        next_element = parent_heading.find_next_sibling()\n",
    "        \n",
    "        ul_tag = None\n",
    "        if next_element:\n",
    "            # Check if the next element is the list itself\n",
    "            if next_element.name == 'ul':\n",
    "                ul_tag = next_element\n",
    "            # Or if it's wrapped in a div\n",
    "            elif next_element.name == 'div':\n",
    "                ul_tag = next_element.find('ul')\n",
    "        \n",
    "        if ul_tag:\n",
    "            characters = ul_tag.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    episode_data['characters'] = characters\n",
    "    \n",
    "except AttributeError:\n",
    "    \n",
    "    episode_data['characters'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Anime notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    anime_notes = None\n",
    "    heading = main_content.find('span', id='Anime_Notes')\n",
    "    \n",
    "    if heading:\n",
    "        # Robustly find the parent heading tag\n",
    "        parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "        \n",
    "        # Find the first <ul> tag that appears after the heading\n",
    "        ul_tag = parent_heading.find_next_sibling('ul')\n",
    "        \n",
    "        if ul_tag:\n",
    "            # For notes, getting direct children (recursive=False) is often cleaner\n",
    "            notes = [li.get_text(strip=True) for li in ul_tag.find_all('li', recursive=False)]\n",
    "            anime_notes = \"\\n\".join(notes)\n",
    "            \n",
    "    episode_data['anime_notes'] = anime_notes\n",
    "\n",
    "except AttributeError:\n",
    "    episode_data['anime_notes'] = None\n",
    "\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trivia = None\n",
    "    heading = main_content.find('span', id='Trivia')\n",
    "    if heading:\n",
    "        ul_tag = heading.find_parent('h2').find_next_sibling('ul')\n",
    "        if ul_tag:\n",
    "            # clean out reference tag\n",
    "            for sup in ul_tag.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            trivia_items = [li.get_text(strip=True) for li in ul_tag.find_all('li', recursive=False)]\n",
    "            trivia = '\\n'.join(trivia_items) if trivia_items else None\n",
    "    episode_data['trivia'] = trivia\n",
    "except AttributeError:\n",
    "    episode_data['trivia'] = None\n",
    "episode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_anime(url, headers=None):\n",
    "    \"\"\"\n",
    "    Fetches and parses a single anime episode page with robust safeguards.\n",
    "    Returns a dictionary of episode data, or None if the page fails to load.\n",
    "    Missing fields within the page will be set to None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            # Return None for pages that don't exist (like future episodes)\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    episode_data = {'url': url}\n",
    "\n",
    "    # From infobox\n",
    "    infobox = soup.find('aside', class_='portable-infobox')\n",
    "    if infobox:\n",
    "        # Episode Number (with fallback)\n",
    "        try:\n",
    "            num_div = infobox.find('div', string='Episode #')\n",
    "            if num_div:\n",
    "                episode_data['episode_number'] = int(num_div.find_next_sibling('div').get_text(strip=True))\n",
    "            else: # Fallback\n",
    "                nav_tag = infobox.find('nav', class_='pi-navigation')\n",
    "                span_tag = nav_tag.find('span', class_='nomobile')\n",
    "                num_match = re.search(r'\\d+', span_tag.get_text(strip=True))\n",
    "                episode_data['episode_number'] = int(num_match.group(0)) if num_match else None\n",
    "        except (AttributeError, ValueError):\n",
    "            episode_data['episode_number'] = None\n",
    "\n",
    "        # Titles and Airdate\n",
    "        try:\n",
    "            title_div = infobox.find('h2', class_='pi-title')\n",
    "            episode_data['episode_title'] = title_div.get_text(strip=True, separator=' ') if title_div else None\n",
    "        except AttributeError:\n",
    "            episode_data['episode_title'] = None\n",
    "        try:\n",
    "            date_div = infobox.find('h3', string='Airdate').find_next_sibling('div')\n",
    "            episode_data['air_date'] = parse(date_div.get_text(strip=True).split('[')[0]).strftime('%Y-%m-%d') if date_div else None\n",
    "        except (AttributeError, ValueError, TypeError):\n",
    "            episode_data['air_date'] = None\n",
    "        try:\n",
    "            chapters_div = infobox.find('h3', string='Chapters').find_next_sibling('div')\n",
    "            episode_data['source_chapters'] = chapters_div.get_text(strip=True, separator=', ') if chapters_div else None\n",
    "        except AttributeError:\n",
    "            episode_data['source_chapters'] = None\n",
    "    else:\n",
    "        episode_data.update({'episode_number': None, 'episode_title': None, 'air_date': None, 'source_chapters': None})\n",
    "\n",
    "    #from Main Content ---\n",
    "    main_content = soup.find('div', class_='mw-parser-output')\n",
    "    if main_content:\n",
    "        def get_summary_text(summary_id):\n",
    "            try:\n",
    "                heading = main_content.find('span', id=summary_id)\n",
    "                if heading:\n",
    "                    summary_ps = []\n",
    "                    parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "                    for sibling in parent_heading.find_next_siblings():\n",
    "                        if sibling.name in ['h2', 'h3']: break\n",
    "                        if sibling.name == 'p':\n",
    "                            summary_ps.append(sibling.get_text(strip=True))\n",
    "                    return \" \".join(summary_ps) if summary_ps else None\n",
    "                return None\n",
    "            except AttributeError: return None\n",
    "\n",
    "        episode_data['short_summary'] = get_summary_text('Short_Summary')\n",
    "        episode_data['long_summary'] = get_summary_text('Long_Summary')\n",
    "\n",
    "        try:\n",
    "            characters = None\n",
    "            heading = main_content.find('span', id='Characters_in_Order_of_Appearance')\n",
    "            if heading:\n",
    "                parent_heading = heading.find_parent(re.compile(r'h[1-6]'))\n",
    "                next_element = parent_heading.find_next_sibling()\n",
    "                ul_tag = None\n",
    "                if next_element:\n",
    "                    if next_element.name == 'ul': ul_tag = next_element\n",
    "                    elif next_element.name == 'div': ul_tag = next_element.find('ul')\n",
    "                if ul_tag: characters = ul_tag.get_text(separator='\\n', strip=True)\n",
    "            episode_data['characters'] = characters\n",
    "        except AttributeError:\n",
    "            episode_data['characters'] = None\n",
    "\n",
    "        try:\n",
    "            notes = None\n",
    "            heading = main_content.find('span', id='Anime_Notes')\n",
    "            if heading:\n",
    "                ul_tag = heading.find_parent(re.compile(r'h[1-6]')).find_next_sibling('ul')\n",
    "                if ul_tag:\n",
    "                    notes_list = [li.get_text(strip=True) for li in ul_tag.find_all('li', recursive=False)]\n",
    "                    notes = \"\\n\".join(notes_list)\n",
    "            episode_data['anime_notes'] = notes\n",
    "        except AttributeError:\n",
    "            episode_data['anime_notes'] = None\n",
    "            \n",
    "        try:\n",
    "            trivia = None\n",
    "            heading = main_content.find('span', id='Trivia')\n",
    "            if heading:\n",
    "                ul_tag = heading.find_parent(re.compile(r'h[1-6]')).find_next_sibling('ul')\n",
    "                if ul_tag:\n",
    "                    for sup in ul_tag.find_all('sup'): sup.decompose()\n",
    "                    trivia_list = [li.get_text(strip=True) for li in ul_tag.find_all('li', recursive=False)]\n",
    "                    trivia = \"\\n\".join(trivia_list)\n",
    "            episode_data['trivia'] = trivia\n",
    "        except AttributeError:\n",
    "            episode_data['trivia'] = None\n",
    "    else:\n",
    "        episode_data.update({'short_summary': None, 'long_summary': None, 'characters': None, 'anime_notes': None, 'trivia': None})\n",
    "\n",
    "    return episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Parse episodes and save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_path = os.path.join(ROOT, 'data')\n",
    "\n",
    "JSON_FILE = 'one_piece_episodes.json'\n",
    "JSON_PATH = os.path.join(data_path, JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_RANGE = range(1, 1143) \n",
    "\n",
    "# load existing data to avoid re-scraping\n",
    "try:\n",
    "    if os.path.exists(JSON_PATH):\n",
    "        with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            all_episodes_data = json.load(f)\n",
    "        existing_urls = {entry['url'] for entry in all_episodes_data if 'url' in entry}\n",
    "    else:\n",
    "        all_episodes_data = []\n",
    "except (json.JSONDecodeError, IOError) as e:\n",
    "    print(f\"Error loading existing data: {e}\")\n",
    "    all_episodes_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_episode_numbers = {ep.get('episode_number') for ep in all_episodes_data}\n",
    "episodes_to_scrape = [num for num in EPISODE_RANGE if num not in scraped_episode_numbers]\n",
    "\n",
    "print(f\"Total episodes to scrape: {len(episodes_to_scrape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = 'https://onepiece.fandom.com/wiki/Episode_{}'\n",
    "scraper_headers = {\n",
    "            'User-Agent': 'OnePieceRAGBot/1.0 (Learning Project; jfcastaneda.led@gmail.com)'\n",
    "        }\n",
    "\n",
    "for ep_num in tqdm(episodes_to_scrape, desc=\"Scraping Episodes\", unit=\"episode\"):\n",
    "    url = BASE_URL.format(ep_num)\n",
    "    if url in existing_urls:\n",
    "        print(f\"Skipping already scraped URL: {url}\")\n",
    "        continue\n",
    "\n",
    "    episode_data = parse_anime(url, headers=scraper_headers)\n",
    "    \n",
    "    # only add if data was successfully fetched\n",
    "    if episode_data and episode_data.get('episode_number') is not None:\n",
    "        all_episodes_data.append(episode_data)\n",
    "        print(f\"Scraped episode {ep_num}: {url}\")\n",
    "    \n",
    "        # Save progress after each successful scrape\n",
    "        try:\n",
    "            with open(JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_episodes_data, f, ensure_ascii=False, indent=4)\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    # Be polite with a short delay\n",
    "    time.sleep(1)  # 1 second delay between requests\n",
    "\n",
    "print(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
